{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-09-09 15:39:41.039244\n"
     ]
    }
   ],
   "source": [
    "%run imports.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "print2 = kmtools.df_tools.print2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "NOTEBOOK_NAME = 'interface_data_statistics'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_remote = datapkg.MySQLConnection(\n",
    "    os.environ['DATAPKG_CONNECTION_STRING'], \n",
    "    NOTEBOOK_NAME, \n",
    "    None, \n",
    "    echo=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "### Load DATA\n",
    "\n",
    "This includes both calculated (with $\\Delta \\Delta G$ prediction) and not calculated (without $\\Delta \\Delta G$ prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'load_data/DATA.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d14aeac475f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load_data/DATA.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mifh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mDATA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mifh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'load_data/DATA.pkl'"
     ]
    }
   ],
   "source": [
    "with open('load_data/DATA.pkl', 'rb') as ifh:\n",
    "    DATA = pickle.load(ifh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in DATA:\n",
    "    if key in ['humsavar', 'clinvar', 'cosmic']:\n",
    "        print(key)\n",
    "        DATA[key] = (\n",
    "            DATA[key][\n",
    "                (DATA[key]['uniprot_id_1'].isnull() | \n",
    "                 ~DATA[key]['uniprot_id_1'].str.contains('-').astype(bool)) &\n",
    "                (DATA[key]['uniprot_id_2'].isnull() | \n",
    "                 ~DATA[key]['uniprot_id_2'].str.contains('-').astype(bool))\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "### Load DATA_DF\n",
    "\n",
    "This only includes calculated mutations (with $\\Delta \\Delta G$ prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_DF = pd.read_pickle('interface_load_data/DATA_DF.pkl')\n",
    "#DATA_DF['ddg_exp'] = DATA_DF['ddg_exp'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DF = (\n",
    "    DATA_DF[\n",
    "        (DATA_DF['uniprot_id_1'].isnull() |\n",
    "         ~DATA_DF['uniprot_id_1'].str.contains('-').astype(bool)) &\n",
    "        (DATA_DF['uniprot_id_2'].isnull() |\n",
    "         ~DATA_DF['uniprot_id_2'].str.contains('-').astype(bool))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load DATA_DF_TT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DF = pd.read_pickle('interface_load_data/DATA_DF_TT.pkl')\n",
    "#DATA_DF['ddg_exp'] = DATA_DF['ddg_exp'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DF_TT = (\n",
    "    DATA_DF_TT[\n",
    "        (DATA_DF_TT['uniprot_id_1'].isnull() |\n",
    "         ~DATA_DF_TT['uniprot_id_1'].str.contains('-').astype(bool)) &\n",
    "        (DATA_DF_TT['uniprot_id_2'].isnull() |\n",
    "         ~DATA_DF_TT['uniprot_id_2'].str.contains('-').astype(bool))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA['cosmic'][['uniprot_id', 'uniprot_mutation']].drop_duplicates().shape\n",
    "# 26907"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_DF[DATA_DF['dataset'] == 'cosmic'][['uniprot_id', 'uniprot_mutation']].drop_duplicates().shape\n",
    "# 26907"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "DATA.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "header_columns = ['kortemme_baker', 'skempi', 'skempi_database']\n",
    "tail_columns = ['humsavar', 'clinvar', 'cosmic']\n",
    "\n",
    "columns_all = (\n",
    "    header_columns + \n",
    "    sorted(c for c in DATA.keys() if c not in header_columns and c not in tail_columns) +\n",
    "    tail_columns\n",
    ")\n",
    "assert not set(DATA.keys()) - set(columns_all)\n",
    "\n",
    "columns_nodiffseqi = (\n",
    "    header_columns + \n",
    "    sorted(c for c in DATA.keys() if not c.endswith('_database') and \n",
    "           c not in header_columns and c not in tail_columns) +\n",
    "    tail_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns_nodiffseqi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "### All columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "counts = {}\n",
    "df_out = pd.DataFrame(columns=columns_all, index=columns_all, dtype=float)\n",
    "\n",
    "for dataset_1 in columns_all:\n",
    "    # print(dataset_1)\n",
    "    df_1 = DATA[dataset_1].copy()\n",
    "    # df_1 = df[df['dataset'] == dataset_1]\n",
    "    mutation_set_1 = set(df_1['uniprot_id'].astype(str) + '.' + df_1['uniprot_mutation'].astype(str))\n",
    "    counts[dataset_1] = len(set(\n",
    "        df_1['pdb_id'].astype(str) + '.' + \n",
    "        df_1['uniprot_id'].astype(str) + '.' + \n",
    "        df_1['uniprot_mutation'].astype(str)))\n",
    "    for dataset_2 in columns_all:\n",
    "        # print('\\t', dataset_2)\n",
    "        df_2 = DATA[dataset_2].copy()\n",
    "        # df_2 = df[df['dataset'] == dataset_2]\n",
    "        mutation_set_2 = set(df_2['uniprot_id'].astype(str) + '.' + df_2['uniprot_mutation'].astype(str))\n",
    "        frac_covered = 1 - len(mutation_set_1 - mutation_set_2) / len(mutation_set_1)\n",
    "        df_out.loc[dataset_1, dataset_2] = frac_covered * 100.0\n",
    "df_out.index = ['{}\\n(n = {:.0f})'.format(x, counts[x]) for x in df_out.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "fg, ax = plt.subplots(figsize=(24, 20))\n",
    "sns.heatmap(df_out, annot=True, fmt=\".2f\", ax=ax, square=True, cbar=False, annot_kws={\"size\": 18})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some validation to make sure that we \n",
    "fg, ax = plt.subplots()\n",
    "ascommon.plotting_tools.make_plot_with_corr(\n",
    "    x='ddg_exp', \n",
    "    y='dg_change',\n",
    "    data=(\n",
    "        DATA_DF\n",
    "        [DATA_DF['dataset'] == 'taipale_ppi']\n",
    "        # .drop_duplicates(subset=['uniprot_id', 'partner_uniprot_id', 'uniprot_mutation'])\n",
    "        .groupby(['uniprot_id', 'partner_uniprot_id', 'uniprot_mutation'])\n",
    "        [['ddg_exp', 'dg_change']]\n",
    "        # .drop_duplicates()\n",
    "        .agg('median')\n",
    "    ),\n",
    "    ax=ax,\n",
    "    corr_type='spearman'\n",
    ")\n",
    "plt.xlabel('PPI no change (0) vs gain or loss (1)')\n",
    "plt.ylabel('FoldX $\\Delta \\Delta G$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some validation to make sure that we \n",
    "fg, ax = plt.subplots()\n",
    "ascommon.plotting_tools.make_plot_with_corr(\n",
    "    x='ddg_exp', \n",
    "    y='dg_change',\n",
    "    data=(\n",
    "        DATA_DF\n",
    "        [DATA_DF['dataset'] == 'taipale_ppi']\n",
    "        # .drop_duplicates(subset=['uniprot_id', 'partner_uniprot_id', 'uniprot_mutation'])\n",
    "        # .groupby(['uniprot_id', 'partner_uniprot_id', 'uniprot_mutation'])\n",
    "        [['ddg_exp', 'dg_change']]\n",
    "        # .drop_duplicates()\n",
    "        # .agg('median')\n",
    "    ),\n",
    "    ax=ax,\n",
    "    corr_type='spearman'\n",
    ")\n",
    "plt.xlabel('PPI no change (0) vs gain or loss (1)')\n",
    "plt.ylabel('FoldX $\\Delta \\Delta G$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Some validation to make sure that we \n",
    "fg, ax = plt.subplots()\n",
    "ascommon.plotting_tools.make_plot_with_corr(\n",
    "    x='ddg_exp', \n",
    "    y='dg_change',\n",
    "    data=(\n",
    "        DATA_DF\n",
    "        [DATA_DF['dataset'] == 'taipale_gpca']\n",
    "        # .drop_duplicates(subset=['uniprot_id', 'partner_uniprot_id', 'uniprot_mutation'])\n",
    "        .groupby(['uniprot_id', 'partner_uniprot_id', 'uniprot_mutation'])\n",
    "        [['ddg_exp', 'dg_change']]\n",
    "        # .drop_duplicates()\n",
    "        .agg('median')\n",
    "    ), \n",
    "    ax=ax,\n",
    "    corr_type='spearman'\n",
    ")\n",
    "plt.xlabel('GPCA score')\n",
    "plt.ylabel('FoldX $\\Delta \\Delta G$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some validation to make sure that we \n",
    "fg, ax = plt.subplots()\n",
    "ascommon.plotting_tools.make_plot_with_corr(\n",
    "    x='ddg_exp', \n",
    "    y='dg_change',\n",
    "    data=(\n",
    "        DATA_DF\n",
    "        [DATA_DF['dataset'] == 'taipale_gpca']\n",
    "        # .drop_duplicates(subset=['uniprot_id', 'partner_uniprot_id', 'uniprot_mutation'])\n",
    "        # .groupby(['uniprot_id', 'partner_uniprot_id', 'uniprot_mutation'])\n",
    "        [['ddg_exp', 'dg_change']]\n",
    "        # .drop_duplicates()\n",
    "        # .agg('median')\n",
    "    ), \n",
    "    ax=ax,\n",
    "    corr_type='spearman'\n",
    ")\n",
    "plt.xlabel('GPCA score')\n",
    "plt.ylabel('FoldX $\\Delta \\Delta G$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "### Correlation with Foldx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "corrs = OrderedDict()\n",
    "for table_name in columns_nodiffseqi:\n",
    "    df = DATA[table_name].drop_duplicates()\n",
    "    if table_name in ['humsavar_train', 'clinvar_train', 'cosmic_train']:\n",
    "        df['ddg_exp'] = df['del_class_exp']\n",
    "    df = df.dropna(subset=['dg_change', 'ddg_exp'])\n",
    "    corrs[table_name] = sp.stats.spearmanr(df['dg_change'], df['ddg_exp'])[0]\n",
    "corrs = pd.Series(corrs)\n",
    "\n",
    "fg, ax = plt.subplots()\n",
    "x = sns.barplot(corrs.values, corrs.index, ax=ax, color=sns.xkcd_rgb[\"denim blue\"])\n",
    "plt.xlim(0, 0.6)\n",
    "ax.set_xlabel('Spearman correlation between experiment\\nand FoldX $\\Delta \\Delta G$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "### Correlation with Provean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "corrs = OrderedDict()\n",
    "for table_name in columns_nodiffseqi:\n",
    "    df = DATA[table_name].drop_duplicates()\n",
    "    if table_name in ['humsavar', 'clinvar', 'cosmic']:\n",
    "        df['ddg_exp'] = df['del_class_exp']\n",
    "    else:\n",
    "        df['ddg_exp'] = df['ddg_exp'].abs()\n",
    "    df = df.dropna(subset=['provean_score', 'ddg_exp'])\n",
    "    corrs[table_name] = -sp.stats.spearmanr(df['provean_score'], df['ddg_exp'])[0]\n",
    "corrs = pd.Series(corrs)\n",
    "\n",
    "fg, ax = plt.subplots()\n",
    "x = sns.barplot(corrs.values, corrs.index, ax=ax, color=sns.xkcd_rgb[\"pale red\"])\n",
    "plt.xlim(0, 0.6)\n",
    "ax.set_xlabel('Spearman correlation between experiment\\nand Provean score (negative)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# DATA_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_DF.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_DF['dataset'].drop_duplicates().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    'skempi',\n",
    "    'taipale_ppi',\n",
    "    'taipale_gpca',\n",
    "    'humsavar',\n",
    "    'clinvar',\n",
    "    'cosmic',\n",
    "    'ab_bind',\n",
    "    'benedix_et_al',\n",
    "    'hiv_escape_mutations',\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "# Histogram sequence identities\n",
    "for dataset in datasets:\n",
    "    plt.figure()\n",
    "    DATA_DF[DATA_DF['dataset'] == dataset]['alignment_identity'].hist(range=(0, 1), bins=10)\n",
    "    plt.title(dataset)\n",
    "    plt.xlabel(\"Fraction sequence identity\")\n",
    "    plt.ylabel(\"Number of mutations\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# Mutaton in domain\n",
    "DATA_DF['mutation_in_domain'] = (\n",
    "    DATA_DF[['uniprot_mutation', 'domain_def']]\n",
    "    .apply(lambda x: ascommon.sequence_tools.mutation_in_domain(*x), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# Mutation in model domain\n",
    "DATA_DF['mutation_in_model_domain'] = (\n",
    "    DATA_DF[['uniprot_mutation', 'model_domain_def']]\n",
    "    .apply(lambda x: ascommon.sequence_tools.mutation_in_domain(*x), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# Mutation to calculate\n",
    "DATA_DF['mutation_to_calculate'] = (\n",
    "    (DATA_DF['ddg'].isnull()) & \n",
    "    (DATA_DF['mutation_in_model_domain'] | (\n",
    "        DATA_DF['mutation_in_model_domain'].isnull() & DATA_DF['mutation_in_domain']))\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "source": [
    "_num_to_calculate = sum(DATA_DF['mutation_to_calculate'])\n",
    "_num_calculated = sum(~DATA_DF['mutation_to_calculate'])\n",
    "assert _num_to_calculate + _num_calculated == DATA_DF.shape[0]\n",
    "\n",
    "print2('Have ddG:', sum(DATA_DF['ddg'].notnull()))\n",
    "print2('Calculated or hopeless:', _num_to_calculate)\n",
    "print2('To calculate:', _num_calculated)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "DATA_DF[DATA_DF['ddg'].isnull()][[\n",
    "    'uniprot_mutation', \n",
    "    'domain_def', 'mutation_in_domain',\n",
    "    'model_domain_def', 'mutation_in_model_domain',\n",
    "    'mutation_to_calculate', 'ddg'\n",
    "]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "## Dataset overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_training_set = [\n",
    "    'skempi', 'taipale_ppi', 'taipale_gpca', 'humsavar', 'clinvar', 'cosmic'\n",
    "]\n",
    "\n",
    "datasets = [\n",
    "    # Train\n",
    "    'skempi',\n",
    "    # \n",
    "    'ab_bind',\n",
    "    'benedix_et_al',\n",
    "    'hiv_escape_mutations',\n",
    "    # Validate\n",
    "    'taipale_ppi',\n",
    "    'taipale_gpca',\n",
    "    # \n",
    "    'humsavar_train',\n",
    "    'clinvar_train',\n",
    "    'cosmic_train',\n",
    "    # Test\n",
    "    'humsavar_test',\n",
    "    'clinvar_test',\n",
    "    'cosmic_test',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unique_id(df):\n",
    "    _df = df[['uniprot_id', 'uniprot_mutation']].dropna()\n",
    "    uniprot_mutation_set = set(_df['uniprot_id'].astype(str) + '.' + _df['uniprot_mutation'].astype(str))\n",
    "    \n",
    "    _df = df[['pdb_id', 'pdb_mutation']].dropna()\n",
    "    pdb_mutation_set = set(_df['pdb_id'].astype(str) + '.' + _df['pdb_mutation'].astype(str))\n",
    "    \n",
    "    return uniprot_mutation_set, pdb_mutation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "counts = {}\n",
    "df = DATA_DF.dropna(subset=['ddg']).copy()\n",
    "df_out = pd.DataFrame(columns=datasets, index=datasets, dtype=float)\n",
    "\n",
    "for dataset_1 in datasets:\n",
    "    # print(dataset_1)\n",
    "    df_1 = df[df['dataset'] == dataset_1]\n",
    "    if df_1.empty:\n",
    "        print(dataset_1)\n",
    "    uniprot_mutation_set_1, pdb_mutation_set_1 = get_unique_id(df_1)\n",
    "    counts[dataset_1] = max([len(uniprot_mutation_set_1), len(pdb_mutation_set_1)])\n",
    "    for dataset_2 in datasets:\n",
    "        # print('\\t', dataset_2)\n",
    "        df_2 = df[df['dataset'] == dataset_2]\n",
    "        if df_2.empty:\n",
    "            print(dataset_2)\n",
    "        uniprot_mutation_set_2, pdb_mutation_set_2 = get_unique_id(df_2)\n",
    "        if uniprot_mutation_set_1:\n",
    "            uniprot_frac_covered = (\n",
    "                1 - len(uniprot_mutation_set_1 - uniprot_mutation_set_2) / len(uniprot_mutation_set_1))\n",
    "        else:\n",
    "            uniprot_frac_covered = 0\n",
    "        if pdb_mutation_set_1:\n",
    "            pdb_frac_covered = (\n",
    "                1 - len(pdb_mutation_set_1 - pdb_mutation_set_2) / len(pdb_mutation_set_1))\n",
    "        else:\n",
    "            pdb_frac_covered = 0\n",
    "        df_out.loc[dataset_1, dataset_2] = max([uniprot_frac_covered, pdb_frac_covered]) * 100.0\n",
    "df_out.index = ['{}\\n(n = {:.0f})'.format(x, counts[x]) for x in df_out.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "fg, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(df_out, annot=True, fmt=\".1f\", ax=ax, square=True, cbar=False, annot_kws={\"size\": 18})\n",
    "plt.yticks(rotation='horizontal')\n",
    "# plt.savefig(op.join(NOTEBOOK_NAME, 'training_set_overlap_final.png'), bbox_inches='tight', dpi=220)\n",
    "# plt.savefig(op.join(NOTEBOOK_NAME, 'training_set_overlap_final.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fg, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(df_out, annot=True, fmt=\".1f\", ax=ax, square=True, cbar=False, annot_kws={\"size\": 18})\n",
    "plt.yticks(rotation='horizontal')\n",
    "plt.savefig(op.join(NOTEBOOK_NAME, 'training_set_overlap_final.png'), bbox_inches='tight', dpi=220)\n",
    "plt.savefig(op.join(NOTEBOOK_NAME, 'training_set_overlap_final.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation with FoldX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "corrs = OrderedDict()\n",
    "for table_name in datasets:\n",
    "    df = DATA_DF[DATA_DF['dataset'] == table_name].drop_duplicates().copy()\n",
    "    if any(table_name.startswith(n) for n in ['humsavar', 'clinvar', 'cosmic']):\n",
    "        df['ddg_exp'] = df['del_class_exp']\n",
    "        # df['dg_change'] = df['dg_change'].abs()\n",
    "    df = df.dropna(subset=['dg_change', 'ddg_exp'])\n",
    "    corrs[table_name] = sp.stats.spearmanr(df['dg_change'], df['ddg_exp'])[0]\n",
    "corrs = pd.Series(corrs)\n",
    "\n",
    "fg, ax = plt.subplots()\n",
    "x = sns.barplot(corrs.values, corrs.index, ax=ax, color=sns.xkcd_rgb[\"denim blue\"])\n",
    "plt.xlim(0, 0.6)\n",
    "ax.set_xlabel('Spearman correlation between experiment\\nand FoldX $\\Delta \\Delta G$')\n",
    "plt.savefig(op.join(NOTEBOOK_NAME, 'foldx_correlation_final.png'), bbox_inches='tight', dpi=220)\n",
    "plt.savefig(op.join(NOTEBOOK_NAME, 'foldx_correlation_final.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation with Provean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "corrs = OrderedDict()\n",
    "for table_name in datasets:\n",
    "    df = DATA_DF[DATA_DF['dataset'] == table_name].drop_duplicates()\n",
    "    if any(table_name.startswith(n) for n in ['humsavar', 'clinvar', 'cosmic']):\n",
    "        df['ddg_exp'] = df['del_class_exp']\n",
    "    else:\n",
    "        df['ddg_exp'] = df['ddg_exp'].abs()\n",
    "        pass\n",
    "    df = df.dropna(subset=['provean_score', 'ddg_exp'])\n",
    "    corrs[table_name] = -sp.stats.spearmanr(df['provean_score'], df['ddg_exp'])[0]\n",
    "corrs = pd.Series(corrs)\n",
    "\n",
    "fg, ax = plt.subplots()\n",
    "x = sns.barplot(corrs.values, corrs.index, ax=ax, color=sns.xkcd_rgb[\"pale red\"])\n",
    "plt.xlim(0, 0.6)\n",
    "ax.set_xlabel('Spearman correlation between experiment\\nand Provean score (negative)')\n",
    "plt.savefig(op.join(NOTEBOOK_NAME, 'provean_correlation_final.png'), bbox_inches='tight', dpi=220)\n",
    "plt.savefig(op.join(NOTEBOOK_NAME, 'provean_correlation_final.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare ELASPIC and FoldX on the test set (NO ABS)\n",
    "\n",
    "plot_datasets = [\n",
    "    'skempi', 'taipale_ppi', 'taipale_gpca', 'humsavar', 'clinvar', 'cosmic'\n",
    "]\n",
    "plot_dataset_names = {\n",
    "    'skempi': 'Skempi',\n",
    "    'taipale_ppi': 'Taipale PPI',\n",
    "    'taipale_gpca': 'Taipale GPCA' ,\n",
    "    'humsavar_train': 'Humsavar',\n",
    "    'clinvar_train': 'Clinvar',\n",
    "    'cosmic_train': 'Cosmic',\n",
    "}\n",
    "\n",
    "fg, axes = plt.subplots(1, 6, figsize=(16, 4))\n",
    "axes[0].set_ylabel('Spearman R')\n",
    "for i, test_dataset in enumerate([\n",
    "        'skempi', 'taipale_ppi', 'taipale_gpca', \n",
    "        'humsavar_train', 'clinvar_train', 'cosmic_train']):\n",
    "    ax = axes[i]\n",
    "    # print(test_dataset)\n",
    "    if test_dataset in ['skempi', 'taipale_ppi', 'taipale_gpca']:\n",
    "        foldx_r = sp.stats.spearmanr(\n",
    "            DATA_DF[DATA_DF['dataset'] == test_dataset]['dg_change'], \n",
    "            DATA_DF[DATA_DF['dataset'] == test_dataset]['ddg_exp'])[0]\n",
    "        provean_r = sp.stats.spearmanr(\n",
    "            DATA_DF[DATA_DF['dataset'] == test_dataset]['provean_score'], \n",
    "            DATA_DF[DATA_DF['dataset'] == test_dataset]['ddg_exp'])[0] * -1\n",
    "    else:\n",
    "        foldx_r = sp.stats.spearmanr(\n",
    "            DATA_DF[DATA_DF['dataset'] == test_dataset]['dg_change'],  # NO ABS!!!\n",
    "            DATA_DF[DATA_DF['dataset'] == test_dataset]['del_class_exp'])[0]\n",
    "        provean_r = sp.stats.spearmanr(\n",
    "            DATA_DF[DATA_DF['dataset'] == test_dataset]['provean_score'], \n",
    "            DATA_DFTT[DATA_DF['dataset'] == test_dataset]['del_class_exp'])[0] * -1\n",
    "    ax.bar(\n",
    "        [1, 2], \n",
    "        [foldx_r, provean_r],\n",
    "        tick_label=['FoldX', 'Provean'], \n",
    "        align='center', \n",
    "        color=[sns.palettes.color_palette()[0], \n",
    "               sns.palettes.color_palette()[4]])\n",
    "    ax.set_ylim(0, 0.60)\n",
    "    ax.set_title(plot_dataset_names[test_dataset])\n",
    "    ax.set_xticklabels(labels=ax.get_xticklabels(), rotation=45)\n",
    "    # print(elaspic_r, foldx_r)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(op.join(NOTEBOOK_NAME, 'initial_performance.png'), bbox_inches='tight', dpi=220)\n",
    "plt.savefig(op.join(NOTEBOOK_NAME, 'initial_performance.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "424px",
   "left": "91.9792px",
   "right": "20px",
   "top": "63.9757px",
   "width": "230px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
